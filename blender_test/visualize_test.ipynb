{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "import sys\n",
    "from bertviz import model_view, head_view\n",
    "import yaml\n",
    "\n",
    "sys.path.insert(0, osp.abspath('.'))\n",
    "\n",
    "import argparse\n",
    "from modules.embedder import *\n",
    "from utils.train_utils import StateCLEVR, ImageCLEVR, ImageCLEVR_HDF5\n",
    "\n",
    "AVAILABLE_DATASETS = {\n",
    "    'DeltaRN': [StateCLEVR],\n",
    "    'DeltaSQFormer': [StateCLEVR],\n",
    "    'DeltaQFormer': [StateCLEVR],\n",
    "    'DeltaSQFormerCross': [StateCLEVR],\n",
    "    'DeltaSQFormerDisentangled': [StateCLEVR],\n",
    "    'DeltaSQFormerLinear': [StateCLEVR],\n",
    "    'DeltaRNFP': [ImageCLEVR, ImageCLEVR_HDF5],\n",
    "}\n",
    "\n",
    "AVAILABLE_MODELS = {'DeltaRN': DeltaRN,\n",
    "                    'DeltaRNFP': DeltaRNFP,\n",
    "                    'DeltaSQFormer': DeltaSQFormer,\n",
    "                    'DeltaSQFormerCross': DeltaSQFormerCross,\n",
    "                    'DeltaSQFormerDisentangled': DeltaSQFormerDisentangled,\n",
    "                    'DeltaSQFormerLinear': DeltaSQFormerLinear,\n",
    "                    'DeltaQFormer': DeltaQFormer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def single_scene_translator(scene: dict, translation: dict, alter_rotation_by=None, alter_position_by=None):\n",
    "    if alter_position_by is None:\n",
    "        alter_position_by = lambda x: x\n",
    "\n",
    "    if alter_rotation_by is None:\n",
    "        alter_rotation_by = lambda x: x\n",
    "\n",
    "    image_index = scene['image_index']\n",
    "    n_objects = len(scene['objects'])\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "    thetas = []\n",
    "    colors = []\n",
    "    materials = []\n",
    "    shapes = []\n",
    "    sizes = []\n",
    "    for obj in scene['objects']:\n",
    "        xs.append(alter_position_by(obj['3d_coords'][0]) / 3)\n",
    "        ys.append(alter_position_by(obj['3d_coords'][1]) / 3)\n",
    "        thetas.append(alter_rotation_by(obj['3d_coords'][2]) / 360)\n",
    "        colors.append(translation[obj['color']])\n",
    "        materials.append(translation[obj['material']])\n",
    "        shapes.append(translation[obj['shape']])\n",
    "        sizes.append(translation[obj['size']])\n",
    "\n",
    "    #######################################################\n",
    "    object_positions_x = torch.FloatTensor(xs + (10 - n_objects) * [0]).unsqueeze(1)\n",
    "    object_positions_y = torch.FloatTensor(ys + (10 - n_objects) * [0]).unsqueeze(1)\n",
    "    object_positions_t = torch.FloatTensor(thetas + (10 - n_objects) * [0]).unsqueeze(1)\n",
    "\n",
    "    object_positions = torch.cat([object_positions_x, object_positions_y, object_positions_t], 1).view(10, 3)\n",
    "    object_colors = torch.LongTensor(colors + (10 - n_objects) * [0])\n",
    "\n",
    "    object_shapes = torch.LongTensor(shapes + (10 - n_objects) * [0])\n",
    "    object_materials = torch.LongTensor(materials + (10 - n_objects) * [0])\n",
    "    object_sizes = torch.LongTensor(sizes + (10 - n_objects) * [0])\n",
    "\n",
    "    return image_index, n_objects, object_positions, object_colors, object_shapes, object_materials, object_sizes\n",
    "\n",
    "\n",
    "def single_question_parser(question: dict, word_replace_dict: dict, q2index: dict, a2index: dict):\n",
    "    image_index = question['image_index']\n",
    "    q = str(question['question'])\n",
    "    a = str(question['answer'])\n",
    "    if word_replace_dict is None:\n",
    "        pass\n",
    "    else:\n",
    "        for word, replacement in word_replace_dict.items():\n",
    "            q = q.replace(word, replacement)\n",
    "            a = a.replace(word, replacement)\n",
    "    if q2index is None:\n",
    "        pass\n",
    "    else:\n",
    "        q = '<START>' + ' ' + q + ' ' + '<END>'\n",
    "        tokenized_q = []\n",
    "        for word in q.split(' '):\n",
    "            if 'bullet' in word or 'butterfly' in word:\n",
    "                return image_index, None, None, None\n",
    "            elif '?' in word or ';' in word:\n",
    "                tokenized_q.append(q2index[word[:-1]])\n",
    "                tokenized_q.append(q2index[';'])\n",
    "            else:\n",
    "                tokenized_q.append(q2index[word])\n",
    "        q = torch.LongTensor(tokenized_q + [0] * (50 - len(tokenized_q))).view(50)\n",
    "    if a2index is None:\n",
    "        pass\n",
    "    else:\n",
    "        a = torch.LongTensor([a2index[a] - 4])\n",
    "\n",
    "    return image_index, len(tokenized_q), q, a\n",
    "\n",
    "\n",
    "def kwarg_dict_to_device(data_obj, device):\n",
    "    if device == 'cpu':\n",
    "        return data_obj\n",
    "    cpy = {}\n",
    "    for key, _ in data_obj.items():\n",
    "        cpy[key] = data_obj[key].to(device)\n",
    "    return cpy\n",
    "\n",
    "\n",
    "def load_encoders():\n",
    "    with open('../translation_tables.yaml', 'r') as fin:\n",
    "        translation = yaml.load(fin, Loader=yaml.FullLoader)['translation']\n",
    "    with open(f'../data/vocab.json', 'r') as fin:\n",
    "        parsed_json = json.load(fin)\n",
    "        q2index = parsed_json['question_token_to_idx']\n",
    "        a2index = parsed_json['answer_token_to_idx']\n",
    "\n",
    "    return translation, q2index, a2index\n",
    "\n",
    "\n",
    "def load_model(device, load_from=None):\n",
    "    if device == 'cuda':\n",
    "        device = 'cuda:0'\n",
    "    experiment_name = load_from.split('results/')[-1].split('/')[0]\n",
    "    config = f'../results/{experiment_name}/config.yaml'\n",
    "    with open(config, 'r') as fin:\n",
    "        config = yaml.load(fin, Loader=yaml.FullLoader)\n",
    "\n",
    "    model = AVAILABLE_MODELS[config['model_architecture']](config)\n",
    "    model = model.to(device)\n",
    "    checkpoint = torch.load(load_from)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loading Model of type: {config['model_architecture']}\\n\")\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def encode_questions_and_scenes(question, q2index, a2index, scene, translation, alter_rotation_by=None,\n",
    "                                alter_position_by=None):\n",
    "    image_index_scene, n_objects, object_positions, object_colors, object_shapes, object_materials, object_sizes = \\\n",
    "        single_scene_translator(scene=scene, translation=translation, alter_rotation_by=alter_rotation_by,\n",
    "                                alter_position_by=alter_position_by)\n",
    "\n",
    "    image_index_question, n_tokens, q, a = single_question_parser(question,\n",
    "                                                                  word_replace_dict={'true': 'yes',\n",
    "                                                                                     'True': 'yes',\n",
    "                                                                                     'false': 'no',\n",
    "                                                                                     'False': 'no'\n",
    "                                                                                     },\n",
    "                                                                  q2index=q2index,\n",
    "                                                                  a2index=a2index)\n",
    "    if n_tokens is None:\n",
    "        return 0, 0\n",
    "\n",
    "    if image_index_scene == image_index_question:\n",
    "        types = [1] * n_objects + [0] * (10 - n_objects) + [2] * n_tokens + [0] * (50 - n_tokens)\n",
    "        types = torch.LongTensor(types).view(60)\n",
    "        positions = torch.LongTensor([0] * 10 + list(range(1, n_tokens + 1)) + [0] * (50 - n_tokens)).view(60)\n",
    "        x = {'positions': positions.unsqueeze(0),\n",
    "             'types': types.unsqueeze(0),\n",
    "             'object_positions': object_positions.unsqueeze(0),\n",
    "             'object_colors': object_colors.unsqueeze(0),\n",
    "             'object_shapes': object_shapes.unsqueeze(0),\n",
    "             'object_materials': object_materials.unsqueeze(0),\n",
    "             'object_sizes': object_sizes.unsqueeze(0),\n",
    "             'question': q.unsqueeze(0),\n",
    "             }\n",
    "        y = a\n",
    "    else:\n",
    "        # print(f\"Image index {image_index_scene} and question index {image_index_question} do not match!\\n\")\n",
    "        return 1, 1\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def fp(model, x, y, device):\n",
    "    with torch.no_grad():\n",
    "        x = kwarg_dict_to_device(x, device=device)\n",
    "        y = y.to(device)\n",
    "        model = model.to(device)\n",
    "        y_pred, att_maps, _ = model(**x)\n",
    "    acc = (y_pred.argmax(1) == y).float().detach().cpu().numpy()\n",
    "    return acc[0], att_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model(device='cuda', load_from='../results/experiment_linear_sq/mos_epoch_28.pt')\n",
    "\n",
    "translation, q2index, a2index = load_encoders()\n",
    "\n",
    "with open(f'../data/CLEVR_val_scenes.json', 'r') as fin:\n",
    "    parsed_json = json.load(fin)\n",
    "    scenes = parsed_json['scenes'][0:10]\n",
    "\n",
    "with open(f'../data/CLEVR_val_questions.json', 'r') as fin:\n",
    "    parsed_json = json.load(fin)\n",
    "    questions = parsed_json['questions'][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "acc = 0\n",
    "eligible = 0\n",
    "question_counter = 10\n",
    "scene_counter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = scenes[scene_counter]\n",
    "object_tokens = []\n",
    "for entry in scene['objects']:\n",
    "    color = entry['color']\n",
    "    size  = entry['size']\n",
    "    material = entry['material']\n",
    "    shape = entry['shape']\n",
    "    object_tokens.append(size + ' ' + color + ' ' + material + ' ' + shape)\n",
    "\n",
    "result = 'y'\n",
    "while result == 'y':\n",
    "    question = questions[question_counter]\n",
    "    print(\"Want to change question? y/n \\n\")\n",
    "    prompt = input().strip()\n",
    "    if prompt == 'n':\n",
    "        q = question['question']\n",
    "    else:\n",
    "        q = prompt\n",
    "    effective_question = ['<START>'] + [f for f in q.split(' ')] + ['<END>']\n",
    "    question_tokens = object_tokens + ['NULL'] * (10 - len(object_tokens)) + effective_question + (50 - len(effective_question)) * ['NULL'] + [f'SP{f+1}' for f in range(4)]\n",
    "    x, y = encode_questions_and_scenes(question, q2index, a2index, scene, translation)\n",
    "\n",
    "    acc_, att_maps = fp(model, x, y, device='cuda')\n",
    "    att_maps = [f.detach().cpu() for f in att_maps]\n",
    "    model_view(att_maps, question_tokens, display_mode=\"light\")\n",
    "    print(\"Want to change question? y/n \\n\")\n",
    "    result = input()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "rapid",
   "display_name": "Python 3.8.8 64-bit ('rapid': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}